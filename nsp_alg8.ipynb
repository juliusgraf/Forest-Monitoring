{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bb2lDkPWA5l",
        "outputId": "8f8f7683-319f-4f4d-d555-718e5719a349"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import gamma\n",
        "from scipy import integrate\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "#!pip install dbfread\n",
        "import dbfread\n",
        "plt.rcParams['text.usetex'] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3mxFEsrrKP9"
      },
      "outputs": [],
      "source": [
        "shape_file = pd.read_csv('final.csv')\n",
        "max_crown_din = shape_file.loc[:, 'maxCrownDiameter']\n",
        "max_cluster_radius = max(max_crown_din) / 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GspYaYjHXFHu"
      },
      "source": [
        "Définition du domaine $K$, hyperparamètres $d,\\Psi, \\alpha, \\beta$  de $p( m, \\omega, {\\Theta})$ et $\\bar{L}(K)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "VaMu_RRkgcEk",
        "outputId": "9a8d85f9-0e3a-40c6-8a0d-ed368d1ddc34"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import invwishart, uniform, multivariate_normal, poisson\n",
        "from scipy.stats import gamma as ga\n",
        "# Hyperparameters\n",
        "# Domain boundaries\n",
        "xmin, xmax = 0, 10\n",
        "ymin, ymax = 0, 10\n",
        "vol = (xmax - xmin) * (ymax - ymin)\n",
        "\n",
        "# Distributions' parameters\n",
        "d = 2\n",
        "cluster_rate = 0.07\n",
        "psi = np.array([[1, 0], [0, 1]], dtype=float)  # Covariance matrix\n",
        "alpha = 18\n",
        "beta = 1\n",
        "\n",
        "alpha_beta = 75\n",
        "beta_beta = 0.5\n",
        "\n",
        "# Generate Theta from an inverse Wishart distribution\n",
        "theta = np.array(invwishart(df=d, scale=psi).rvs())\n",
        "\n",
        "# Generate omega from a Gamma distribution\n",
        "# omega = float(ga(a=alpha, scale=1/beta).rvs())\n",
        "\n",
        "# Generate m from a uniform distribution over the domain\n",
        "L = int(poisson(cluster_rate*vol).rvs())\n",
        "# Number of clusters and points per cluster\n",
        "num_clusters = L\n",
        "max_cluster_radius = 2.5\n",
        "# Initialize list to hold all points\n",
        "X = []\n",
        "true_cluster_centers = []\n",
        "\n",
        "def generate_cluster_points(cluster_center, theta, points_per_cluster, xmin, xmax, ymin, ymax):\n",
        "    points = []\n",
        "    while len(points) < points_per_cluster:\n",
        "        new_points = np.array(multivariate_normal(mean=cluster_center, cov=theta).rvs(size=points_per_cluster))\n",
        "        for point in new_points:\n",
        "            if xmin <= point[0] <= xmax and ymin <= point[1] <= ymax:\n",
        "                points.append(point)\n",
        "                if len(points) >= points_per_cluster:\n",
        "                    break\n",
        "    return np.array(points)\n",
        "\n",
        "# Generate points for each cluster\n",
        "for _ in range(num_clusters):\n",
        "    # Each cluster center m is drawn from a uniform distribution\n",
        "    cluster_center = np.array(uniform(loc=[xmin, ymin], scale=[xmax - xmin, ymax - ymin]).rvs())\n",
        "    true_cluster_centers.append(cluster_center)\n",
        "    # Generate points around the cluster center using a multivariate normal distribution\n",
        "    theta = np.array(invwishart(df=d, scale=psi).rvs())\n",
        "    points_per_cluster = int(ga(a=alpha, scale=1/beta).rvs())\n",
        "    cluster_points = generate_cluster_points(cluster_center, theta, points_per_cluster, xmin, xmax, ymin, ymax)\n",
        "    X.append(cluster_points)\n",
        "\n",
        "# Stack all the points into a single array\n",
        "X = np.vstack(X)\n",
        "\n",
        "# Display the generated points\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "plt.scatter(X[:, 0], X[:, 1], c='blue', label='Data Points')\n",
        "plt.xlim(xmin, xmax)\n",
        "plt.ylim(ymin, ymax)\n",
        "plt.title(\"Generated Points following an NSP with anisotropic Gaussian impulse response\")\n",
        "plt.xlabel(\"Axe $x$\")\n",
        "plt.ylabel(\"Axe $y$\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrW0579KpOXf"
      },
      "source": [
        "Estimateur de Monte Carlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i1PXpwApbH4"
      },
      "outputs": [],
      "source": [
        "def volume(domain):\n",
        "  domain = np.array(domain)\n",
        "  delta_domain = domain[1] - domain[0]\n",
        "  return np.prod(delta_domain)\n",
        "\n",
        "def monte_carlo_integration(function, domain, n_iterations, **kwargs):\n",
        "     \"\"\"\n",
        "     TODO\n",
        "     \"\"\"\n",
        "     domain = np.array(domain)\n",
        "     x_list = np.random.default_rng().uniform(domain[0], domain[1], [n_iterations, 2])\n",
        "     f = np.apply_along_axis(function, 1, x_list, **kwargs)\n",
        "     integral_estim = f.sum() * volume(domain) / f.shape[0]\n",
        "\n",
        "     return integral_estim\n",
        "\n",
        "\n",
        "# Experimeting a different implementation for the Monte Carlo integration\n",
        "def MC_integration(function, domain, n_iterations, **kwargs):\n",
        "    \"\"\"\n",
        "    TODO\n",
        "    \"\"\"\n",
        "    domain = np.array(domain)\n",
        "    x_list = np.random.default_rng().uniform(domain[0], domain[1], [n_iterations, 2])\n",
        "    f = function(x_list, **kwargs)\n",
        "\n",
        "    integral_estim = f.sum() * volume(domain) / f.shape[0]\n",
        "\n",
        "    return integral_estim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaadDFNcugBc"
      },
      "source": [
        "Définition de l'estimation de la matrice variance-covariance $\\hat{V}$ d'un cluster et de la fonction $\\phi$\n",
        "\n",
        "$$ \\hat{V} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i-m)(x_i-m)^\\top$$\n",
        "\n",
        "$$ \\phi(m,x) = \\Psi + N\\times \\hat{V}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXeR2Nt6uqIO"
      },
      "outputs": [],
      "source": [
        "def cluster_cov_matrix_estimation(m, cluster_points):\n",
        "  \"\"\"\n",
        "  TODO\n",
        "  \"\"\"\n",
        "  N, dim = cluster_points.shape\n",
        "  diff_mx_transp = m - cluster_points.reshape((N, 1, dim))\n",
        "  diff_mx = np.transpose(diff_mx_transp, axes=[0, 2, 1])\n",
        "\n",
        "  V = np.matmul(\n",
        "      diff_mx, diff_mx_transp, axes=[(-2, -1), (-2, -1), (1, 2)]\n",
        "  )\n",
        "\n",
        "  return V.sum(axis=0) / N\n",
        "\n",
        "# Experimenting a new implementation for the cluster cov matrix\n",
        "def cluster_cov_matrix_estimation_(m, cluster_points):\n",
        "        \"\"\"\n",
        "        TODO\n",
        "        \"\"\"\n",
        "        N = cluster_points.shape[0]\n",
        "        V = np.zeros((m.shape[0], m.shape[1], m.shape[1]))\n",
        "        for i in range(N):\n",
        "            diff_mx = m.reshape((m.shape[0], m.shape[1], 1)) - cluster_points[i, :].reshape(cluster_points.shape[1], 1)\n",
        "            diff_mx_transp = m.reshape((m.shape[0], 1, m.shape[1])) - cluster_points[i, :].reshape(cluster_points.shape[1], 1).T\n",
        "            V += np.matmul(\n",
        "                diff_mx, diff_mx_transp, axes=[(-2, -1), (-2, -1), (1, 2)]\n",
        "            )\n",
        "\n",
        "        return V / N\n",
        "\n",
        "\n",
        "def phi(m, x):\n",
        "  \"\"\"\n",
        "  TODO\n",
        "  \"\"\"\n",
        "  N, _ = x.shape\n",
        "  return psi + cluster_cov_matrix_estimation(m, x) * N\n",
        "\n",
        "def phi_(m, x):\n",
        "  \"\"\"\n",
        "  TODO\n",
        "  \"\"\"\n",
        "  N, _ = x.shape\n",
        "  return psi + cluster_cov_matrix_estimation_(m, x) * N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWpecFXC8_9N"
      },
      "source": [
        "Implementation de l'évidence $p(x_n,y_n)$ qui marche pour un seul point $(x_n,y_n)$ et pour multi-dimensionelle $(X_k,Y_k)$.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9u4l9jiXXoW"
      },
      "outputs": [],
      "source": [
        "def evidence(x, n_points = 1, n_iterations = int(1e3)):\n",
        "  \"\"\"\n",
        "  TODO\n",
        "  \"\"\"\n",
        "  if n_points == 1:\n",
        "    x = x.reshape((1, 2))\n",
        "\n",
        "  n_points, dim = x.shape\n",
        "  bounds = [[xmin, ymin], [xmax, ymax]]\n",
        "\n",
        "  def integrand(m, x):\n",
        "    \"\"\"\n",
        "    TODO\n",
        "    \"\"\"\n",
        "    n_points, dim = x.shape\n",
        "    numerator = (\n",
        "        np.linalg.det(psi) ** (d / 2.)\n",
        "        * gamma((d + n_points) / 2.)\n",
        "    )\n",
        "\n",
        "    det_phi = np.linalg.det(phi_(m, x))\n",
        "    denominator = (\n",
        "        np.power(det_phi, (d + n_points) / 2.)\n",
        "        * (np.pi ** n_points) * cluster_rate * volume(bounds)\n",
        "        * gamma(d / 2.)\n",
        "    )\n",
        "\n",
        "    return numerator / denominator\n",
        "\n",
        "  return MC_integration(\n",
        "      integrand, bounds, n_iterations, x = x\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap6lTd_YXbbj"
      },
      "source": [
        "Définition de la prédiction $p(( x_n, y_n)\\mid ({X}_k,{Y}_k))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txAbhw7I9by4"
      },
      "outputs": [],
      "source": [
        "def predictor(x, cluster, n_iterations):\n",
        "  \"\"\"\n",
        "  TODO\n",
        "  \"\"\"\n",
        "  x = x.reshape((1, 2))\n",
        "  bounds = [[xmin, ymin], [xmax, ymax]]\n",
        "\n",
        "  def log_integrand(m, x):\n",
        "    \"\"\"\n",
        "    TODO\n",
        "    \"\"\"\n",
        "    det_phi = np.linalg.det(phi_(m, x))\n",
        "    return np.exp(-((d + x.shape[0]) / 2.) * np.log(det_phi))\n",
        "\n",
        "  # Integral of 1/sigma\n",
        "  z = np.concatenate((x, cluster), axis=0)\n",
        "  sigma_integral = MC_integration(\n",
        "      log_integrand, bounds, n_iterations, x = z\n",
        "  )\n",
        "\n",
        "  # Integral of 1/phi\n",
        "  phi_integral = MC_integration(\n",
        "      log_integrand, bounds, n_iterations, x = cluster\n",
        "  )\n",
        "\n",
        "  numerator = gamma((d + cluster.shape[0] + 1) / 2.) * sigma_integral\n",
        "  denominator = np.pi * gamma((d + cluster.shape[0]) / 2.) * phi_integral\n",
        "\n",
        "  return numerator / denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x6RLkcJy5bL"
      },
      "outputs": [],
      "source": [
        "def sp_evidence(x):\n",
        "    z = x.reshape((1, 2))\n",
        "\n",
        "    def integrand(x, y):\n",
        "        m = np.array([x, y])\n",
        "        m = m.reshape((1, 2))\n",
        "\n",
        "        det_phi = np.linalg.det(phi(m, z))\n",
        "        return np.exp(-((d + 1) / 2.) * np.log(det_phi))\n",
        "\n",
        "    phi_integral, _  = integrate.dblquad(integrand, xmin, xmax, ymin, ymax)\n",
        "\n",
        "    numerator = np.linalg.det(psi) ** (d / 2.) * gamma((d + 1) / 2.)\n",
        "    numerator *= phi_integral\n",
        "\n",
        "    bounds = [[xmin, ymin], [xmax, ymax]]\n",
        "    denominator = np.pi  * cluster_rate * volume(bounds) * gamma(d / 2.)\n",
        "\n",
        "    return numerator / denominator\n",
        "\n",
        "\n",
        "def sp_predictor(x, cluster):\n",
        "    def integrand(x, y):\n",
        "        m = np.array([x, y])\n",
        "        m = m.reshape((1, 2))\n",
        "\n",
        "        det_phi = np.linalg.det(phi(m, cluster))\n",
        "        return np.exp(-((d + cluster.shape[0]) / 2.) * np.log(det_phi))\n",
        "\n",
        "    x = x.reshape((1, 2))\n",
        "    phi_integral, _  = integrate.dblquad(integrand, xmin, xmax, ymin, ymax)\n",
        "\n",
        "    cluster = np.concatenate((x, cluster), axis=0)\n",
        "    sigma_integral, _ = integrate.dblquad(integrand, xmin, xmax, ymin, ymax)\n",
        "\n",
        "    numerator = gamma((d + cluster.shape[0]) / 2.) * sigma_integral\n",
        "    denominator = np.pi * gamma((d + cluster.shape[0] - 1) / 2.) * phi_integral\n",
        "\n",
        "    return numerator / denominator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6fnvP3d0kbS"
      },
      "source": [
        "Testing the convergence of the evidence estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "OQjY2-eI0pk_",
        "outputId": "b0db5904-ff7a-432d-d2b3-6e45e411673a"
      },
      "outputs": [],
      "source": [
        "x = X[33]\n",
        "n_iterations = int(1e5)\n",
        "evidence_result = evidence(x, n_iterations=n_iterations)\n",
        "predictor_result = predictor(x, X, n_iterations)\n",
        "\n",
        "x = np.array([1, 2])\n",
        "convergence = [evidence(x, n_iterations = i) for i in range(1,500)]\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "plt.plot(range(1,500),convergence)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('$\\widehat{p}_n(x)$ as a function of $n$')\n",
        "plt.title('Monte-Carlo integration to approach $p(x)$ by $\\widehat{p}_n(x)$')\n",
        "plt.show()\n",
        "\n",
        "print(\"Evidence:\", evidence_result)\n",
        "print(\"Predictor:\", predictor_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RRDTEUkQ-ov"
      },
      "source": [
        "Different types of maximum cluster radius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRkxSp7JRmOx"
      },
      "outputs": [],
      "source": [
        "def v_max_cluster_radius(m, cluster_points):\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  V = cluster_cov_matrix_estimation(m, cluster_points)\n",
        "  return V.diagonal().max()\n",
        "\n",
        "\n",
        "def too_far(m, x):\n",
        "    dist = np.linalg.norm(m - x)\n",
        "\n",
        "    return dist > max_cluster_radius"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-BPJnapXint"
      },
      "source": [
        "Algorithme 8 (Collapsed Gibbs Sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDf-FsdxXwiW"
      },
      "outputs": [],
      "source": [
        "# Gibbs sampling\n",
        "def gibbs_sampling(X, n_samples, n_iterations, alpha, beta):\n",
        "    N = len(X)\n",
        "    samples = dict()\n",
        "    weights = []\n",
        "    betas = []\n",
        "\n",
        "    for s in tqdm(range(n_samples)):\n",
        "        clusters = dict(zip(list(range(N)), ['background'] * N))  # Initialize all points in the background\n",
        "        cluster_points = {'background': [tuple(x) for x in X]}  # Store points in clusters\n",
        "        lst = np.array(range(N))\n",
        "        np.random.shuffle(lst)\n",
        "\n",
        "        for i in lst:\n",
        "            x_i = tuple(X[i])\n",
        "            current_cluster = clusters[i]\n",
        "            cluster_points[current_cluster].remove(x_i)  # Remove the point from its current cluster\n",
        "\n",
        "            if not cluster_points[current_cluster]:  # Remove the cluster if it's empty\n",
        "                del cluster_points[current_cluster]\n",
        "\n",
        "            # Compute probabilities for existing clusters\n",
        "            prob_existing_clusters = []\n",
        "            for k in cluster_points.keys():\n",
        "                m = np.mean(cluster_points[k], axis=0)\n",
        "                if k != 'background' and len(cluster_points[k]) > 0 and not too_far(m, X[i]):\n",
        "                    prob = (len(cluster_points[k]) + alpha) * predictor(np.array(x_i), np.array(cluster_points[k]), n_iterations)\n",
        "                    prob_existing_clusters.append(prob)\n",
        "                else:\n",
        "                    prob_existing_clusters.append(0)\n",
        "\n",
        "            # Compute probability for new cluster\n",
        "            prob_new_cluster = alpha * evidence(np.array([x_i]), n_iterations=n_iterations) * (beta / (1 + beta)) ** alpha\n",
        "\n",
        "            # Compute probability for background cluster\n",
        "            prob_background = 0.\n",
        "\n",
        "            # Normalize probabilities\n",
        "            probs = prob_existing_clusters + [prob_new_cluster, prob_background]\n",
        "            probs = np.array(probs)\n",
        "            #print(probs)\n",
        "            #np.nan_to_num(probs, copy=False)\n",
        "            probs = probs / np.sum(probs)\n",
        "\n",
        "            # Sample new cluster assignment\n",
        "            np.random.seed(i * N + n_samples * s)\n",
        "            new_cluster = np.random.choice(len(probs), p=probs, replace=False)\n",
        "\n",
        "            # Assign point to new cluster\n",
        "            if new_cluster == len(probs) - 2:  # New cluster\n",
        "                new_cluster_id = max([int(k) for k in cluster_points.keys() if k != 'background']) + 1 if any(k != 'background' for k in cluster_points.keys()) else 0\n",
        "                clusters[i] = str(new_cluster_id)\n",
        "                cluster_points[str(new_cluster_id)] = [x_i]\n",
        "            elif new_cluster == len(probs) - 1:  # Background cluster\n",
        "                clusters[i] = 'background'\n",
        "                if 'background' in cluster_points:\n",
        "                    cluster_points['background'].append(x_i)\n",
        "                else:\n",
        "                    cluster_points['background'] = [x_i]\n",
        "            else:  # Existing cluster\n",
        "                cluster_id = list(cluster_points.keys())[new_cluster]\n",
        "                clusters[i] = cluster_id\n",
        "                cluster_points[cluster_id].append(x_i)\n",
        "        \n",
        "        # Sample weights for each cluster\n",
        "        cluster_weights = {}\n",
        "        for k in cluster_points.keys():\n",
        "            if k != 'background':\n",
        "                cluster_weights[k] = float(ga(a=alpha + len(cluster_points[k]), scale=1 / beta).rvs())\n",
        "            else: #i.e. if it's empty\n",
        "                cluster_weights[k] = float(ga(a=alpha, scale=1/(1+beta)).rvs())\n",
        "        \n",
        "        E = float(poisson(vol*(beta/(1+beta))**(alpha)).rvs())\n",
        "        L = len(cluster_points) + E\n",
        "        beta_sampled = float(ga(a=alpha_beta + L*alpha, scale = 1/(beta_beta + sum(cluster_weights.values()))).rvs())\n",
        "        \n",
        "        samples[s] = cluster_points\n",
        "        weights.append(cluster_weights)\n",
        "        betas.append(beta_sampled)\n",
        "\n",
        "    return samples, weights, betas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QyElJPQRpcM"
      },
      "source": [
        "Application of the test case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha_beta = 165\n",
        "beta_beta = 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "j1gRn2hcRPo2",
        "outputId": "d85befc3-16a6-49e7-9a78-0dc159c39b68"
      },
      "outputs": [],
      "source": [
        "# Application of the test case\n",
        "n_samples = 5000\n",
        "n_iterations = 300\n",
        "samples, weights, betas = gibbs_sampling(X, n_samples, n_iterations, alpha, beta)\n",
        "print(samples)\n",
        "#print(\"Cluster Assignments:\", cluster_points)\n",
        "#print(\"Clusters:\", clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** we plot $\\widehat{\\beta}^{\\mu}_n+0.03$ in order to circumvent the optimal parametrization of $\\alpha_\\beta$ and $\\beta_\\beta$. The latter would allow to effectively obersve convergence towards the right limit $\\beta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the running average of the betas\n",
        "running_avg_betas = np.cumsum(betas) / np.arange(1, len(betas) + 1)\n",
        "\n",
        "# Plot the convergence\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "plt.plot(range(1, len(running_avg_betas) + 1), running_avg_betas+0.03, label=r'Posterior estimation of $\\beta$')\n",
        "plt.axhline(y=beta, color='r', linestyle='--', label=r'$\\beta$')\n",
        "plt.xlabel(r'Number of samples $n$')\n",
        "plt.ylabel(r'Value of $\\displaystyle \\widehat{\\beta}^{\\mu}_n$')\n",
        "plt.title(r'Convergence of $\\displaystyle \\widehat{\\beta}^{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^{n} \\widehat{\\beta}_{i}$')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** we plot $${\\widehat{\\gamma}_n^{\\mu}}^*=\\displaystyle\\frac{\\alpha}{\\widehat{\\beta}^{\\mu}_n+0.03}$$ instead of $\\widehat{\\gamma}_n^{\\mu} = \\alpha/\\widehat{\\beta}^{\\mu}_n$ in order to circumvent the optimal parametrization of $\\alpha_\\beta$ and $\\beta_\\beta$. The latter would allow to effectively obersve convergence towards the right limit $\\alpha/\\beta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the running average of the betas\n",
        "a_b = [alpha/(betas_val+0.035) for betas_val in betas]\n",
        "running_avg_alpha_betas = np.cumsum(a_b) / np.arange(1, len(betas) + 1)\n",
        "\n",
        "# Plot the convergence\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "plt.plot(range(1, len(running_avg_betas) + 1), running_avg_alpha_betas, label=r'Posterior estimation of $\\gamma:=\\alpha/\\beta$')\n",
        "plt.axhline(y=alpha/beta, color='r', linestyle='--', label=r'$\\gamma=\\alpha/\\beta$')\n",
        "plt.xlabel(r'Number of samples $n$')\n",
        "plt.ylabel(r'Value of $\\displaystyle \\widehat{\\gamma}^{\\mu}_n$')\n",
        "plt.title(r'Convergence of $\\displaystyle \\widehat{\\gamma}_n^{\\mu} = \\frac{\\alpha}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n} \\widehat{\\beta}_{i}}$')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "mPUeBtX6dctz",
        "outputId": "5aeff5a8-beb7-4538-a0bf-0e020381646a"
      },
      "outputs": [],
      "source": [
        "def samples_histogram(samples):\n",
        "    h = [len(cluster) for cluster in samples.values()]\n",
        "    plt.figure(figsize=(10, 6), dpi=300)\n",
        "    plt.hist(h)\n",
        "    plt.xlabel('Réalisations $|\\mathcal{C}|(\\omega)$')\n",
        "    plt.ylabel('Nombre de réalisations')\n",
        "    plt.title('Histogramme du nombre de clusters non-vides $|\\mathcal{C}|$')\n",
        "\n",
        "def cluster_plot(cluster):\n",
        "    plt.figure(figsize=(10, 6), dpi=300)\n",
        "    plt.xlim(xmin, xmax)\n",
        "    plt.ylim(ymin, ymax)\n",
        "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray', 'cyan', 'magenta']\n",
        "\n",
        "    for cluster_id, points in cluster.items():\n",
        "        if cluster_id == 'background':\n",
        "            color = 'black'\n",
        "            marker = 'x'\n",
        "            label = 'Background'\n",
        "        else:\n",
        "            color = colors[int(cluster_id) % len(colors)]\n",
        "            marker = 'o'\n",
        "            label = f'Cluster {cluster_id}'\n",
        "\n",
        "        points = np.array(points)\n",
        "        plt.scatter(points[:, 0], points[:, 1], c=color, marker=marker, label=label)\n",
        "\n",
        "        # Compute and plot the convex hull\n",
        "        if len(points) > 2:  # Convex hull cannot be computed for fewer than 3 points\n",
        "            hull = ConvexHull(points)\n",
        "            for simplex in hull.simplices:\n",
        "                plt.plot(points[simplex, 0], points[simplex, 1], color=color)\n",
        "\n",
        "    plt.title('Clustering $\\displaystyle \\mathcal{C} = \\{\\mathcal{C}_k; |\\mathcal{C}_k|>0\\}$')\n",
        "    plt.xlabel('Axe $x_1$')\n",
        "    plt.ylabel('Axe $x_2$')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#for s in samples:\n",
        "#    cluster_plot(samples[s])\n",
        "cluster_plot(samples[0])\n",
        "cluster_plot(samples[1250])\n",
        "cluster_plot(samples[2750])\n",
        "#cluster_plot(samples[1000])\n",
        "#cluster_plot(samples[4000])\n",
        "samples_histogram(samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_cluster = [cluster for cluster in samples.values() if len(cluster) == 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_plot(k_cluster[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_max = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mean_positions(samples, k_max=k_max):\n",
        "    \n",
        "    k_cluster = [cluster for cluster in samples.values() if len(cluster) == k_max]\n",
        "    mean_positions = {i: [] for i in range(k_max)}\n",
        "\n",
        "    def calculate_centroid(points):\n",
        "        return np.mean(points, axis=0)\n",
        "\n",
        "    for cluster in k_cluster:\n",
        "        # Compute centroids for all clusters\n",
        "        centroids = {k: calculate_centroid(v) for k, v in cluster.items()}\n",
        "        sorted_keys = sorted(centroids.keys(), key=lambda k: (centroids[k][0], centroids[k][1]))\n",
        "        \n",
        "        for i, key in enumerate(sorted_keys):\n",
        "            mean_positions[i].extend(cluster[key])\n",
        "\n",
        "    mean_positions_result = {}\n",
        "    for i in range(k_max):\n",
        "        if len(mean_positions[i]) > 0:\n",
        "            mean_positions_result[i] = np.mean(mean_positions[i], axis=0)\n",
        "        else:\n",
        "            mean_positions_result[i] = np.zeros(len(mean_positions[i][0]))\n",
        "    return mean_positions_result\n",
        "\n",
        "def plot_mean_positions(mean_positions):\n",
        "    plt.figure(figsize=(10, 6), dpi=300)\n",
        "    \n",
        "    for i in range(len(mean_positions)):\n",
        "        position = mean_positions[i]\n",
        "        plt.scatter(position[0], position[1], label=f'Cluster {i}', s=100)\n",
        "    \n",
        "    # Plot true cluster centers\n",
        "    for i, center in enumerate(true_cluster_centers):\n",
        "        plt.scatter(center[0], center[1], s=100, marker='x', color='black')\n",
        "    \n",
        "    plt.xlabel('$x$ coordinate')\n",
        "    plt.ylabel('$y$ coordinate')\n",
        "    plt.title('Estimated mean positions $(\\widehat{m}_\\ell)_{1\\leq \\ell \\leq |\\mathcal{C}|}$ and $\\\\times$ true centroids')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "mean_positions = compute_mean_positions(samples, k_max)\n",
        "print(mean_positions)\n",
        "plot_mean_positions(mean_positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list1 = list(mean_positions.values())\n",
        "list2 = true_cluster_centers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Set k_max\n",
        "inf = min(len(list1), len(list2))  # Ensure k_max does not exceed the length of the lists\n",
        "\n",
        "list1 = list(mean_positions.values())[:inf]\n",
        "list2 = true_cluster_centers[:inf]\n",
        "\n",
        "# Calculate pairwise distances\n",
        "distances = np.zeros((len(list1), len(list2)))\n",
        "for i, point1 in enumerate(list1):\n",
        "    for j, point2 in enumerate(list2):\n",
        "        distances[i, j] = np.linalg.norm(point1 - point2)\n",
        "\n",
        "# Start with the point whose distance to the second set is minimum\n",
        "associations = []\n",
        "used_indices_list2 = set()\n",
        "for _ in range(len(list1)):\n",
        "    min_distance = float('inf')\n",
        "    min_index_list1 = -1\n",
        "    min_index_list2 = -1\n",
        "    \n",
        "    for i in range(len(list1)):\n",
        "        for j in range(len(list2)):\n",
        "            if j in used_indices_list2:\n",
        "                continue\n",
        "            if distances[i, j] < min_distance:\n",
        "                min_distance = distances[i, j]\n",
        "                min_index_list1 = i\n",
        "                min_index_list2 = j\n",
        "                \n",
        "    associations.append((min_index_list1, min_index_list2))\n",
        "    used_indices_list2.add(min_index_list2)\n",
        "    \n",
        "# Compute the sum of the squared distances\n",
        "sum_squared_distances = 0\n",
        "for i, j in associations:\n",
        "    dist = np.linalg.norm(list1[i] - list2[j])\n",
        "    sum_squared_distances += dist ** 2\n",
        "    print(f\"Point {list1[i]} from list1 is closest to point {list2[j]} from list2 with squared distance {dist ** 2}\")\n",
        "\n",
        "# Calculate the mean of the squared distances and then the square root\n",
        "mean_squared_distance = sum_squared_distances / len(associations)\n",
        "sqrt_mean_squared_distance = np.sqrt(mean_squared_distance)\n",
        "\n",
        "print(f\"Square root of the mean of the squared distances: {sqrt_mean_squared_distance}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
